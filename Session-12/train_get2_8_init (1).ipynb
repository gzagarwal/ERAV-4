{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "56589a13",
      "metadata": {
        "id": "56589a13"
      },
      "source": [
        "# Train GPT-style model (converted from train_get2-8-init.py)\n",
        "# Notebook generated from script: train_get2-8-init.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67faddd2",
      "metadata": {
        "id": "67faddd2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Note: we'll create 'enc' (tokenizer) in a later cell so it's available globally in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a812aca",
      "metadata": {
        "id": "5a812aca"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b802bff",
      "metadata": {
        "id": "7b802bff"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257\n",
        "        config_args['block_size'] = 1024\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
        "\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca73bfb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca73bfb6",
        "outputId": "4560fe1f-16a7-40f3-8c96-f3ea4b319108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# device selection and seed\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "# SEED\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "# STOP / generation settings\n",
        "num_return_sequences = 2\n",
        "max_length = 30\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7817a2be",
      "metadata": {
        "id": "7817a2be"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # at init load tokens from disk and store them in memory\n",
        "        with open('input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f'loaded {len(self.tokens)} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "\n",
        "        # state\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B*T\n",
        "        # if loading the next batch would be out of bounds, reset\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4691ec12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4691ec12",
        "outputId": "193ef457-4b8d-47df-8f7f-45150b34e5b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 338025 tokens\n",
            "1 epoch = 660 batches\n"
          ]
        }
      ],
      "source": [
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "\n",
        "train_loader = DataLoaderLite(B=2, T=256)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9111c93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9111c93",
        "outputId": "f7314433-7e15-43be-cb7b-d62ba312ef72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, loss 10.9737\n",
            "step 500, loss 5.8927\n",
            "step 1000, loss 5.0836\n",
            "step 1500, loss 4.0982\n",
            "step 2000, loss 4.4014\n",
            "step 2500, loss 3.9711\n",
            "step 3000, loss 4.2580\n",
            "step 3500, loss 3.2198\n",
            "step 4000, loss 2.6338\n",
            "step 4500, loss 2.7354\n",
            "step 5000, loss 3.1169\n",
            "step 5500, loss 2.3285\n",
            "step 6000, loss 1.7150\n",
            "step 6500, loss 1.3034\n",
            "step 7000, loss 1.4597\n",
            "step 7500, loss 1.2713\n",
            "step 8000, loss 1.0071\n",
            "step 8500, loss 0.8056\n",
            "step 9000, loss 0.8944\n",
            "step 9500, loss 0.7389\n",
            "step 10000, loss 0.3259\n",
            "step 10500, loss 0.6052\n",
            "step 11000, loss 0.3293\n",
            "step 11500, loss 0.2114\n",
            "step 12000, loss 0.2597\n",
            "step 12500, loss 0.3367\n",
            "step 13000, loss 0.4185\n",
            "step 13500, loss 0.3198\n",
            "step 14000, loss 0.1842\n",
            "step 14500, loss 0.2720\n",
            "step 15000, loss 0.1987\n",
            "step 15500, loss 0.1855\n",
            "step 16000, loss 0.1946\n",
            "step 16500, loss 0.2589\n",
            "step 17000, loss 0.2184\n",
            "step 17500, loss 0.1686\n",
            "step 18000, loss 0.1969\n",
            "step 18500, loss 0.2635\n",
            "step 19000, loss 0.1279\n",
            "step 19500, loss 0.1452\n",
            "step 20000, loss 0.1500\n",
            "step 20500, loss 0.1190\n",
            "step 21000, loss 0.1597\n",
            "step 21500, loss 0.1940\n",
            "step 22000, loss 0.1153\n",
            "step 22500, loss 0.1117\n",
            "step 23000, loss 0.1246\n",
            "step 23500, loss 0.1094\n",
            "step 24000, loss 0.1113\n",
            "step 24500, loss 0.1035\n",
            "step 25000, loss 0.0955\n",
            "step 25500, loss 0.2260\n",
            "step 26000, loss 0.1592\n",
            "step 26500, loss 0.0952\n",
            "step 27000, loss 0.1376\n",
            "step 27500, loss 0.1166\n",
            "step 28000, loss 0.0725\n",
            "step 28500, loss 0.1080\n",
            "step 29000, loss 0.1144\n",
            "step 29500, loss 0.1888\n",
            "step 30000, loss 0.1694\n",
            "step 30500, loss 0.0799\n",
            "step 31000, loss 0.1569\n",
            "step 31500, loss 0.0756\n",
            "step 32000, loss 0.0660\n",
            "step 32500, loss 0.0887\n",
            "step 33000, loss 0.1338\n",
            "step 33500, loss 0.1877\n",
            "step 34000, loss 0.1052\n",
            "step 34500, loss 0.1039\n",
            "final loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Training loop (small demo). NOTE: in the original script there was a `sys.exit(0)` here; in the notebook we keep that as a comment so subsequent cells can run.\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=0.0\n",
        ")\n",
        "for i in range(35000):\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    logits, loss = model(x, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 500 == 0:\n",
        "        print(f\"step {i}, loss {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "print('final loss:', loss)\n",
        "# original script had: import sys; sys.exit(0)\n",
        "# keep the training result and continue to generation in later cells\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1cdd37d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1cdd37d",
        "outputId": "417fa548-2f60-4e51-b646-fe15178976b9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \n",
            "Condition!\n",
            "I would I were a Roman; for I cannot,\n",
            "Being a Volsce, be that I am. Condition!\n",
            "\n",
            ">  rotten privilege and custom 'gainst\n",
            "My hate to Marcius: where I find him, were it\n",
            "At home, upon my brother's guard\n"
          ]
        }
      ],
      "source": [
        "# Generation / sampling loop\n",
        "# Set deterministic seed for generation\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# create a starting context x by taking a batch from the train_loader\n",
        "x, y = train_loader.next_batch()\n",
        "\n",
        "# ensure we run on the model device\n",
        "x = x.to(device)\n",
        "\n",
        "while x.size(1) < max_length:\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)[0]\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        ix = torch.multinomial(topk_probs, 1)\n",
        "        xcol = torch.gather(topk_indices, -1, ix)\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3118b6db",
      "metadata": {
        "id": "3118b6db"
      },
      "source": [
        "# Notes\n",
        "# - Open this notebook in VS Code and run cells in order.\n",
        "# - If you wish to run training on GPU, ensure you have CUDA and change batch sizes.\n",
        "# - Consider adding validation split, checkpointing, and LR scheduling for real training.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}