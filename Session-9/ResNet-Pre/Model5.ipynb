{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "caaac7fd",
      "metadata": {
        "id": "caaac7fd"
      },
      "source": [
        "# Iteration 1\n",
        "This notebook demonstrates the `train` and `test` functions for the dataset. The focus is to setup the basic model to have a good accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd283fa6",
      "metadata": {
        "id": "bd283fa6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ca6db051",
      "metadata": {
        "id": "ca6db051"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from models.resnet50 import Net\n",
        "from train_test.train import train, train_losses, train_acc,AlbumentationsWrapper\n",
        "\n",
        "from train_test.train import train_aug,train_transforms, test_transforms,test_aug,resume_if_available,load_checkpoint,get_latest_checkpoint,save_checkpoint\n",
        "from train_test.test import test, test_losses\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "de8066c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def build_tiny_imagenet_val_per_class(root):\n",
        "    val_dir = Path(root)/\"val\"\n",
        "    images_dir = val_dir/\"images\"\n",
        "    ann_path = val_dir/\"val_annotations.txt\"\n",
        "    out_root = Path(root)/\"val_per_class\"\n",
        "\n",
        "    if out_root.exists():\n",
        "        return str(out_root)\n",
        "\n",
        "    out_root.mkdir(parents=True, exist_ok=True)\n",
        "    mapping = {}\n",
        "    with open(ann_path) as f:\n",
        "        for line in f:\n",
        "            img, cls, *_ = line.strip().split(\"\\t\")\n",
        "            mapping[img] = cls\n",
        "\n",
        "    for img_name, cls in mapping.items():\n",
        "        src = images_dir/img_name\n",
        "        dst_dir = out_root/cls/\"images\"\n",
        "        dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "        # use hardlink/symlink to save space if you prefer\n",
        "        shutil.copy2(src, dst_dir/img_name)\n",
        "    return str(out_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bef27215",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 100000\n",
            "Val samples: 10000\n",
            "Number of classes: 200\n"
          ]
        }
      ],
      "source": [
        "# Add this cell - Tiny ImageNet is much smaller (200 classes, 64x64 images)\n",
        "import wget\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def download_tiny_imagenet(data_dir='./data'):\n",
        "    \"\"\"\n",
        "    Download Tiny ImageNet (200 classes, 64x64 images)\n",
        "    Total size: ~240MB\n",
        "    \"\"\"\n",
        "    url = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
        "    zip_path = os.path.join(data_dir, 'tiny-imagenet-200.zip')\n",
        "    extract_path = os.path.join(data_dir, 'tiny-imagenet-200')\n",
        "    \n",
        "    if not os.path.exists(extract_path):\n",
        "        print(\"Downloading Tiny ImageNet...\")\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "        \n",
        "        # Download\n",
        "        wget.download(url, zip_path)\n",
        "        \n",
        "        # Extract\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_dir)\n",
        "        \n",
        "        os.remove(zip_path)  # Clean up zip file\n",
        "        print(f\"\\nTiny ImageNet downloaded to {extract_path}\")\n",
        "    \n",
        "    return extract_path\n",
        "\n",
        "# Use it\n",
        "tiny_imagenet_path = download_tiny_imagenet()\n",
        "val_root = build_tiny_imagenet_val_per_class(tiny_imagenet_path)\n",
        "\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    root=os.path.join(tiny_imagenet_path, 'train'),\n",
        "    transform=None\n",
        ")\n",
        "\n",
        "val_dataset = datasets.ImageFolder(\n",
        "    root=val_root,\n",
        "    transform=None\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Val samples: {len(val_dataset)}\")\n",
        "print(f\"Number of classes: {len(train_dataset.classes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78060b3d",
      "metadata": {
        "id": "78060b3d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "52331208",
      "metadata": {
        "id": "52331208"
      },
      "outputs": [],
      "source": [
        "# Dataset and DataLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_ds = AlbumentationsWrapper(train_dataset, train_aug)\n",
        "test_ds  = AlbumentationsWrapper(val_dataset,  test_aug)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,num_workers=0)\n",
        "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False,num_workers=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "88a0f845",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88a0f845",
        "outputId": "73bc763c-832a-4104-fe86-a49da85f0abc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# CIFAR-10 stats used in your Normalize\n",
        "MEAN = (0.4914, 0.4822, 0.4465)\n",
        "STD  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "def show_batch(tensors, labels=None, n=20):\n",
        "    \"\"\"\n",
        "    tensors: (B, C, H, W) normalized by MEAN/STD\n",
        "    labels:  (B,) optional\n",
        "    \"\"\"\n",
        "    if isinstance(tensors, np.ndarray):\n",
        "            tensors = torch.from_numpy(tensors)\n",
        "    if tensors.dim() == 3:\n",
        "        tensors = tensors.unsqueeze(0)\n",
        "        if labels is not None and not isinstance(labels, (list, tuple)):\n",
        "            labels = [labels]\n",
        "    b = min(n, tensors.size(0))\n",
        "    mean = torch.tensor(MEAN, device=tensors.device).view(1, 3, 1, 1)\n",
        "    std  = torch.tensor(STD,  device=tensors.device).view(1, 3, 1, 1)\n",
        "\n",
        "    # de-normalize to [0,1] range\n",
        "    imgs = tensors * std + mean\n",
        "    imgs = imgs.clamp(0, 1)  # safe for display\n",
        "\n",
        "    cols = 10\n",
        "    rows = (b + cols - 1) // cols\n",
        "    plt.figure(figsize=(1.6*cols, 1.6*rows))\n",
        "    for i in range(b):\n",
        "        plt.subplot(rows, cols, i+1)\n",
        "        img = imgs[i].permute(1, 2, 0).detach().cpu().numpy()  # HWC\n",
        "        plt.imshow(img)\n",
        "        if labels is not None:\n",
        "            plt.title(int(labels[i]))\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()#Let us visualize few sample images\n",
        "\n",
        "\n",
        "#batch_data, batch_label = next(iter(train_loader))\n",
        "#print(batch_data.shape)  # Should be (batch_size, 3, 32, 32)\n",
        "#show_batch(batch_data, batch_label, n=20)\n",
        "\n",
        "#@show_batch(batch_data, batch_label, n=20)\n",
        "#fig = plt.figure(figsize=(15, 10))  # Increased figure size\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "231a50eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "231a50eb",
        "outputId": "235eabf4-0bed-43fd-c966-349cd6a78060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 122, 122]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 122, 122]             128\n",
            "              ReLU-3         [-1, 64, 122, 122]               0\n",
            "           Dropout-4         [-1, 64, 122, 122]               0\n",
            "         MaxPool2d-5           [-1, 64, 61, 61]               0\n",
            "            Conv2d-6           [-1, 64, 61, 61]           4,096\n",
            "       BatchNorm2d-7           [-1, 64, 61, 61]             128\n",
            "              ReLU-8           [-1, 64, 61, 61]               0\n",
            "           Dropout-9           [-1, 64, 61, 61]               0\n",
            "           Conv2d-10           [-1, 64, 61, 61]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 61, 61]             128\n",
            "             ReLU-12           [-1, 64, 61, 61]               0\n",
            "          Dropout-13           [-1, 64, 61, 61]               0\n",
            "           Conv2d-14          [-1, 256, 61, 61]          16,384\n",
            "      BatchNorm2d-15          [-1, 256, 61, 61]             512\n",
            "           Conv2d-16          [-1, 256, 61, 61]          16,384\n",
            "           Conv2d-17          [-1, 256, 61, 61]          16,384\n",
            "      BatchNorm2d-18          [-1, 256, 61, 61]             512\n",
            "      BatchNorm2d-19          [-1, 256, 61, 61]             512\n",
            "             ReLU-20          [-1, 256, 61, 61]               0\n",
            "       Bottleneck-21          [-1, 256, 61, 61]               0\n",
            "           Conv2d-22           [-1, 64, 61, 61]          16,384\n",
            "      BatchNorm2d-23           [-1, 64, 61, 61]             128\n",
            "             ReLU-24           [-1, 64, 61, 61]               0\n",
            "          Dropout-25           [-1, 64, 61, 61]               0\n",
            "           Conv2d-26           [-1, 64, 61, 61]          36,864\n",
            "      BatchNorm2d-27           [-1, 64, 61, 61]             128\n",
            "             ReLU-28           [-1, 64, 61, 61]               0\n",
            "          Dropout-29           [-1, 64, 61, 61]               0\n",
            "           Conv2d-30          [-1, 256, 61, 61]          16,384\n",
            "      BatchNorm2d-31          [-1, 256, 61, 61]             512\n",
            "             ReLU-32          [-1, 256, 61, 61]               0\n",
            "       Bottleneck-33          [-1, 256, 61, 61]               0\n",
            "           Conv2d-34           [-1, 64, 61, 61]          16,384\n",
            "      BatchNorm2d-35           [-1, 64, 61, 61]             128\n",
            "             ReLU-36           [-1, 64, 61, 61]               0\n",
            "          Dropout-37           [-1, 64, 61, 61]               0\n",
            "           Conv2d-38           [-1, 64, 61, 61]          36,864\n",
            "      BatchNorm2d-39           [-1, 64, 61, 61]             128\n",
            "             ReLU-40           [-1, 64, 61, 61]               0\n",
            "          Dropout-41           [-1, 64, 61, 61]               0\n",
            "           Conv2d-42          [-1, 256, 61, 61]          16,384\n",
            "      BatchNorm2d-43          [-1, 256, 61, 61]             512\n",
            "             ReLU-44          [-1, 256, 61, 61]               0\n",
            "       Bottleneck-45          [-1, 256, 61, 61]               0\n",
            "           Conv2d-46          [-1, 128, 61, 61]          32,768\n",
            "      BatchNorm2d-47          [-1, 128, 61, 61]             256\n",
            "             ReLU-48          [-1, 128, 61, 61]               0\n",
            "          Dropout-49          [-1, 128, 61, 61]               0\n",
            "           Conv2d-50          [-1, 128, 31, 31]         147,456\n",
            "      BatchNorm2d-51          [-1, 128, 31, 31]             256\n",
            "             ReLU-52          [-1, 128, 31, 31]               0\n",
            "          Dropout-53          [-1, 128, 31, 31]               0\n",
            "           Conv2d-54          [-1, 512, 31, 31]          65,536\n",
            "      BatchNorm2d-55          [-1, 512, 31, 31]           1,024\n",
            "           Conv2d-56          [-1, 512, 31, 31]         131,072\n",
            "           Conv2d-57          [-1, 512, 31, 31]         131,072\n",
            "      BatchNorm2d-58          [-1, 512, 31, 31]           1,024\n",
            "      BatchNorm2d-59          [-1, 512, 31, 31]           1,024\n",
            "             ReLU-60          [-1, 512, 31, 31]               0\n",
            "       Bottleneck-61          [-1, 512, 31, 31]               0\n",
            "           Conv2d-62          [-1, 128, 31, 31]          65,536\n",
            "      BatchNorm2d-63          [-1, 128, 31, 31]             256\n",
            "             ReLU-64          [-1, 128, 31, 31]               0\n",
            "          Dropout-65          [-1, 128, 31, 31]               0\n",
            "           Conv2d-66          [-1, 128, 31, 31]         147,456\n",
            "      BatchNorm2d-67          [-1, 128, 31, 31]             256\n",
            "             ReLU-68          [-1, 128, 31, 31]               0\n",
            "          Dropout-69          [-1, 128, 31, 31]               0\n",
            "           Conv2d-70          [-1, 512, 31, 31]          65,536\n",
            "      BatchNorm2d-71          [-1, 512, 31, 31]           1,024\n",
            "             ReLU-72          [-1, 512, 31, 31]               0\n",
            "       Bottleneck-73          [-1, 512, 31, 31]               0\n",
            "           Conv2d-74          [-1, 128, 31, 31]          65,536\n",
            "      BatchNorm2d-75          [-1, 128, 31, 31]             256\n",
            "             ReLU-76          [-1, 128, 31, 31]               0\n",
            "          Dropout-77          [-1, 128, 31, 31]               0\n",
            "           Conv2d-78          [-1, 128, 31, 31]         147,456\n",
            "      BatchNorm2d-79          [-1, 128, 31, 31]             256\n",
            "             ReLU-80          [-1, 128, 31, 31]               0\n",
            "          Dropout-81          [-1, 128, 31, 31]               0\n",
            "           Conv2d-82          [-1, 512, 31, 31]          65,536\n",
            "      BatchNorm2d-83          [-1, 512, 31, 31]           1,024\n",
            "             ReLU-84          [-1, 512, 31, 31]               0\n",
            "       Bottleneck-85          [-1, 512, 31, 31]               0\n",
            "           Conv2d-86          [-1, 128, 31, 31]          65,536\n",
            "      BatchNorm2d-87          [-1, 128, 31, 31]             256\n",
            "             ReLU-88          [-1, 128, 31, 31]               0\n",
            "          Dropout-89          [-1, 128, 31, 31]               0\n",
            "           Conv2d-90          [-1, 128, 31, 31]         147,456\n",
            "      BatchNorm2d-91          [-1, 128, 31, 31]             256\n",
            "             ReLU-92          [-1, 128, 31, 31]               0\n",
            "          Dropout-93          [-1, 128, 31, 31]               0\n",
            "           Conv2d-94          [-1, 512, 31, 31]          65,536\n",
            "      BatchNorm2d-95          [-1, 512, 31, 31]           1,024\n",
            "             ReLU-96          [-1, 512, 31, 31]               0\n",
            "       Bottleneck-97          [-1, 512, 31, 31]               0\n",
            "           Conv2d-98          [-1, 256, 31, 31]         131,072\n",
            "      BatchNorm2d-99          [-1, 256, 31, 31]             512\n",
            "            ReLU-100          [-1, 256, 31, 31]               0\n",
            "         Dropout-101          [-1, 256, 31, 31]               0\n",
            "          Conv2d-102          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-103          [-1, 256, 16, 16]             512\n",
            "            ReLU-104          [-1, 256, 16, 16]               0\n",
            "         Dropout-105          [-1, 256, 16, 16]               0\n",
            "          Conv2d-106         [-1, 1024, 16, 16]         262,144\n",
            "     BatchNorm2d-107         [-1, 1024, 16, 16]           2,048\n",
            "          Conv2d-108         [-1, 1024, 16, 16]         524,288\n",
            "          Conv2d-109         [-1, 1024, 16, 16]         524,288\n",
            "     BatchNorm2d-110         [-1, 1024, 16, 16]           2,048\n",
            "     BatchNorm2d-111         [-1, 1024, 16, 16]           2,048\n",
            "            ReLU-112         [-1, 1024, 16, 16]               0\n",
            "      Bottleneck-113         [-1, 1024, 16, 16]               0\n",
            "          Conv2d-114          [-1, 256, 16, 16]         262,144\n",
            "     BatchNorm2d-115          [-1, 256, 16, 16]             512\n",
            "            ReLU-116          [-1, 256, 16, 16]               0\n",
            "         Dropout-117          [-1, 256, 16, 16]               0\n",
            "          Conv2d-118          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-119          [-1, 256, 16, 16]             512\n",
            "            ReLU-120          [-1, 256, 16, 16]               0\n",
            "         Dropout-121          [-1, 256, 16, 16]               0\n",
            "          Conv2d-122         [-1, 1024, 16, 16]         262,144\n",
            "     BatchNorm2d-123         [-1, 1024, 16, 16]           2,048\n",
            "            ReLU-124         [-1, 1024, 16, 16]               0\n",
            "      Bottleneck-125         [-1, 1024, 16, 16]               0\n",
            "          Conv2d-126          [-1, 256, 16, 16]         262,144\n",
            "     BatchNorm2d-127          [-1, 256, 16, 16]             512\n",
            "            ReLU-128          [-1, 256, 16, 16]               0\n",
            "         Dropout-129          [-1, 256, 16, 16]               0\n",
            "          Conv2d-130          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-131          [-1, 256, 16, 16]             512\n",
            "            ReLU-132          [-1, 256, 16, 16]               0\n",
            "         Dropout-133          [-1, 256, 16, 16]               0\n",
            "          Conv2d-134         [-1, 1024, 16, 16]         262,144\n",
            "     BatchNorm2d-135         [-1, 1024, 16, 16]           2,048\n",
            "            ReLU-136         [-1, 1024, 16, 16]               0\n",
            "      Bottleneck-137         [-1, 1024, 16, 16]               0\n",
            "          Conv2d-138          [-1, 256, 16, 16]         262,144\n",
            "     BatchNorm2d-139          [-1, 256, 16, 16]             512\n",
            "            ReLU-140          [-1, 256, 16, 16]               0\n",
            "         Dropout-141          [-1, 256, 16, 16]               0\n",
            "          Conv2d-142          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-143          [-1, 256, 16, 16]             512\n",
            "            ReLU-144          [-1, 256, 16, 16]               0\n",
            "         Dropout-145          [-1, 256, 16, 16]               0\n",
            "          Conv2d-146         [-1, 1024, 16, 16]         262,144\n",
            "     BatchNorm2d-147         [-1, 1024, 16, 16]           2,048\n",
            "            ReLU-148         [-1, 1024, 16, 16]               0\n",
            "      Bottleneck-149         [-1, 1024, 16, 16]               0\n",
            "          Conv2d-150          [-1, 256, 16, 16]         262,144\n",
            "     BatchNorm2d-151          [-1, 256, 16, 16]             512\n",
            "            ReLU-152          [-1, 256, 16, 16]               0\n",
            "         Dropout-153          [-1, 256, 16, 16]               0\n",
            "          Conv2d-154          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-155          [-1, 256, 16, 16]             512\n",
            "            ReLU-156          [-1, 256, 16, 16]               0\n",
            "         Dropout-157          [-1, 256, 16, 16]               0\n",
            "          Conv2d-158         [-1, 1024, 16, 16]         262,144\n",
            "     BatchNorm2d-159         [-1, 1024, 16, 16]           2,048\n",
            "            ReLU-160         [-1, 1024, 16, 16]               0\n",
            "      Bottleneck-161         [-1, 1024, 16, 16]               0\n",
            "          Conv2d-162          [-1, 256, 16, 16]         262,144\n",
            "     BatchNorm2d-163          [-1, 256, 16, 16]             512\n",
            "            ReLU-164          [-1, 256, 16, 16]               0\n",
            "         Dropout-165          [-1, 256, 16, 16]               0\n",
            "          Conv2d-166          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-167          [-1, 256, 16, 16]             512\n",
            "            ReLU-168          [-1, 256, 16, 16]               0\n",
            "         Dropout-169          [-1, 256, 16, 16]               0\n",
            "          Conv2d-170         [-1, 1024, 16, 16]         262,144\n",
            "     BatchNorm2d-171         [-1, 1024, 16, 16]           2,048\n",
            "            ReLU-172         [-1, 1024, 16, 16]               0\n",
            "      Bottleneck-173         [-1, 1024, 16, 16]               0\n",
            "          Conv2d-174          [-1, 512, 16, 16]         524,288\n",
            "     BatchNorm2d-175          [-1, 512, 16, 16]           1,024\n",
            "            ReLU-176          [-1, 512, 16, 16]               0\n",
            "         Dropout-177          [-1, 512, 16, 16]               0\n",
            "          Conv2d-178            [-1, 512, 8, 8]       2,359,296\n",
            "     BatchNorm2d-179            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-180            [-1, 512, 8, 8]               0\n",
            "         Dropout-181            [-1, 512, 8, 8]               0\n",
            "          Conv2d-182           [-1, 2048, 8, 8]       1,048,576\n",
            "     BatchNorm2d-183           [-1, 2048, 8, 8]           4,096\n",
            "          Conv2d-184           [-1, 2048, 8, 8]       2,097,152\n",
            "          Conv2d-185           [-1, 2048, 8, 8]       2,097,152\n",
            "     BatchNorm2d-186           [-1, 2048, 8, 8]           4,096\n",
            "     BatchNorm2d-187           [-1, 2048, 8, 8]           4,096\n",
            "            ReLU-188           [-1, 2048, 8, 8]               0\n",
            "      Bottleneck-189           [-1, 2048, 8, 8]               0\n",
            "          Conv2d-190            [-1, 512, 8, 8]       1,048,576\n",
            "     BatchNorm2d-191            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-192            [-1, 512, 8, 8]               0\n",
            "         Dropout-193            [-1, 512, 8, 8]               0\n",
            "          Conv2d-194            [-1, 512, 8, 8]       2,359,296\n",
            "     BatchNorm2d-195            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-196            [-1, 512, 8, 8]               0\n",
            "         Dropout-197            [-1, 512, 8, 8]               0\n",
            "          Conv2d-198           [-1, 2048, 8, 8]       1,048,576\n",
            "     BatchNorm2d-199           [-1, 2048, 8, 8]           4,096\n",
            "            ReLU-200           [-1, 2048, 8, 8]               0\n",
            "      Bottleneck-201           [-1, 2048, 8, 8]               0\n",
            "          Conv2d-202            [-1, 512, 8, 8]       1,048,576\n",
            "     BatchNorm2d-203            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-204            [-1, 512, 8, 8]               0\n",
            "         Dropout-205            [-1, 512, 8, 8]               0\n",
            "          Conv2d-206            [-1, 512, 8, 8]       2,359,296\n",
            "     BatchNorm2d-207            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-208            [-1, 512, 8, 8]               0\n",
            "         Dropout-209            [-1, 512, 8, 8]               0\n",
            "          Conv2d-210           [-1, 2048, 8, 8]       1,048,576\n",
            "     BatchNorm2d-211           [-1, 2048, 8, 8]           4,096\n",
            "            ReLU-212           [-1, 2048, 8, 8]               0\n",
            "      Bottleneck-213           [-1, 2048, 8, 8]               0\n",
            "AdaptiveAvgPool2d-214           [-1, 2048, 1, 1]               0\n",
            "          Conv2d-215            [-1, 200, 1, 1]         409,600\n",
            "================================================================\n",
            "Total params: 26,694,208\n",
            "Trainable params: 26,694,208\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.68\n",
            "Forward/backward pass size (MB): 417.19\n",
            "Params size (MB): 101.83\n",
            "Estimated Total Size (MB): 519.70\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "# Model, device, optimizer\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net(num_classes=200).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "summary(model, input_size=(3, 244, 244))  # Adjust input size for ImageNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6d5b2f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6d5b2f2",
        "outputId": "c9a39103-79e5-4b7f-f426-bbb14e356d1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=9.436583518981934 Batch_id=1 Accuracy=0.00:   0%|          | 2/1563 [03:18<43:31:38, 100.38s/it]"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "def run(model, optimizer, train_loader, val_loader, epochs=50, ckpt_dir=\"./checkpoints\",\n",
        "        device=\"cuda\", scheduler=None):\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    model.to(device)\n",
        "\n",
        "    # default if none provided\n",
        "    if scheduler is None:\n",
        "        scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    # resume + carry over previous best\n",
        "    best_val_acc = 0.0\n",
        "    latest = get_latest_checkpoint(ckpt_dir)\n",
        "    if latest is not None:\n",
        "        try:\n",
        "            chk = torch.load(latest, map_location=\"cpu\")\n",
        "            best_val_acc = float(chk.get(\"best_val_acc\", 0.0))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    start_epoch = resume_if_available(ckpt_dir, model, optimizer, scheduler)\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        print(f\"\\nEPOCH: {epoch}\")\n",
        "\n",
        "        train_loss, train_acc_epoch = train(model, device, train_loader, optimizer, epoch)\n",
        "        val_loss, val_acc = test(model, device, val_loader)\n",
        "\n",
        "        # step LR (use val_loss if your scheduler needs a metric)\n",
        "        try:\n",
        "            scheduler.step()\n",
        "        except TypeError:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "        save_checkpoint(\n",
        "            ckpt_dir, model, optimizer, epoch, scheduler,\n",
        "            train_loss=float(train_loss),\n",
        "            train_acc=float(train_acc_epoch),\n",
        "            val_loss=float(val_loss),\n",
        "            val_acc=float(val_acc),\n",
        "            best_val_acc=float(best_val_acc),\n",
        "        )\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = float(val_acc)\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"model_state\": model.state_dict(),\n",
        "                    \"optimizer_state\": optimizer.state_dict(),\n",
        "                    \"scheduler_state\": scheduler.state_dict(),\n",
        "                    \"val_acc\": best_val_acc,\n",
        "                    \"best_val_acc\": best_val_acc,\n",
        "                },\n",
        "                os.path.join(ckpt_dir, \"best.pth\"),\n",
        "            )\n",
        "\n",
        "        print(\n",
        "            f\"[Epoch {epoch}] \"\n",
        "            f\"train_loss={train_loss:.4f}  train_acc={train_acc_epoch:.2f}%  \"\n",
        "            f\"val_loss={val_loss:.4f}    val_acc={val_acc:.2f}%  \"\n",
        "            f\"best={best_val_acc:.2f}%\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c537d08",
      "metadata": {},
      "outputs": [],
      "source": [
        "run(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=test_loader,\n",
        "    epochs=50,\n",
        "    ckpt_dir=\"./checkpoints\",\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8075825f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "8075825f",
        "outputId": "3e65b943-bb23-434a-a727-c2618bbe9823"
      },
      "outputs": [],
      "source": [
        "# Plotting results\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axs = plt.subplots(2,2,figsize=(15,10))\n",
        "axs[0, 0].plot([t.item() for t in train_losses])\n",
        "axs[0, 0].set_title(\"Training Loss\")\n",
        "axs[1, 0].plot(train_acc)\n",
        "axs[1, 0].set_title(\"Training Accuracy\")\n",
        "axs[0, 1].plot(test_losses)\n",
        "axs[0, 1].set_title(\"Test Loss\")\n",
        "axs[1, 1].plot(test_acc)\n",
        "axs[1, 1].set_title(\"Test Accuracy\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
